{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bluedrive Data Engineer Assessment\n",
    "\n",
    "Project Requirements:\n",
    "\n",
    "1. Download the dataset Vehicle_sales_data.csv from Kaggle\n",
    "2. Setup a PostgreSQL instance using Docker Compose\n",
    "3. Design a suitable star schema for the dataset and create the tables and relationships in your PostgreSQL instance \n",
    "4. Create an ELT/ETL pipeline to extract, transform, and load the data into the tables in your PostgresSQL instance\n",
    "5. Document the entire process in a Jupyter Notebook\n",
    "6. Test the integrity and accuracy of the data in the PostgreSQL instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tools and Prerequisites:\n",
    "\n",
    "1. WSL2: Ubuntu-24.04 - the general environment for developing the project\n",
    "2. Docker Desktop - for running the PostgreSQL instance\n",
    "3. Kaggle API - for fetching the dataset\n",
    "4. Pandas - library to be used in ETL pipeline\n",
    "5. PostgreSQL Client - for communicating with the PSQL instance using the terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1. Downloading the dataset from Kaggle\n",
    "\n",
    "    a. Sign up for a Kaggle account in https://www.kaggle.com/ and create your API Token. This will be used to download our dataset from the terminal with API calls\n",
    "\n",
    "    b. Download your API Token, named kaggle.json and copy it to this directory '~/.config/kaggle'\n",
    "\n",
    "    c. Install Kaggle API using the command: 'pip install kaggle'\n",
    "    \n",
    "    d. Run the command: 'kaggle datasets download syedanwarafridi/vehicle-sales-data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install the kaggle API\n",
    "%pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the kaggle API by displaying the avaialble datasets\n",
    "#if there are errors, make sure that you have copied the kaggle.json to the ~/.config/kaggle/ folder\n",
    "!kaggle datasets list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download the needed dataset, unzip, and rename it to Vehicle_sales_data.csv\n",
    "#if you don't have the bluedrive_project in your home directory, create it first before running this code\n",
    "!kaggle datasets download syedanwarafridi/vehicle-sales-data -p ~/bluedrive_project --unzip \\\n",
    "&& mv ~/bluedrive_project/car_prices.csv  ~/bluedrive_project/Vehicle_sales_data.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2. Set up a PostgreSQL instance using Docker Compose\n",
    "\n",
    "a. Install Docker Desktop from: https://www.docker.com/products/docker-desktop/\n",
    "b. Setup your Docker Desktop to run WSL Integration. Check out the isntructions here: https://docs.docker.com/desktop/features/wsl/\n",
    "c. Create the docker-compose.yml file\n",
    "d. Create a .env file within the folder where the yml file is located to store sensitive information about the PSQL instance \n",
    "e. Run the command 'docker-compose up' inside the the folder where the yml file is located. Make sure Docker Desktop is running when doing this.\n",
    "f. Test the PSQL instance by connecting to it using psql-client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the docker-compose.yml file for creating a PostgreSQL instance. Copy the code below and save it as 'docker-compose.yml' in '~/bluedrive_project/' folder\n",
    "\n",
    "services:\n",
    "  db:\n",
    "    image: postgres:16  # Use the desired version of PostgreSQL, 16 was used because the latest version for psql client is only at 16\n",
    "    container_name: postgres_instance\n",
    "    environment:\n",
    "      POSTGRES_USER: ${POSTGRES_USER}       # Username for the PostgreSQL instance stored in .env\n",
    "      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}  # Password for the PostgreSQL instance stored in .env\n",
    "      POSTGRES_DB: ${POSTGRES_DB}      # Name of the database to create stored in .env    \n",
    "    ports:\n",
    "      - \"5433:5432\"  # Expose PostgreSQL on the host machine's port 5432"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a sample .env file with containing the sensitive information. Edit as needed and save on the same folder as the yml file.\n",
    "\n",
    "POSTGRES_USER=yourusername\n",
    "POSTGRES_PASSWORD=yourpassword\n",
    "POSTGRES_DB=yourdatabase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the PSQL instance by running the code below\n",
    "!cd ~/bluedrive_project/\n",
    "!docker-compose up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now that there is a running instance of the PSQL in Docker, connect to it using psql-client. Edit yoursudopassword as needed\n",
    "!echo yoursudopassword | sudo -S apt install postgresql-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#connect to your PSQL instance. Remember the credentials set earlier in the .env file and use those to login\n",
    "!psql -U yourusername -h localhost -p 5433 -d yourdatabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop the PSQL instance\n",
    "!cd ~/bluedrive_project/\n",
    "!docker-compose down\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3. Design a star schema suitable for the dataset and create tables and their relationships in the PSQL instance\n",
    "\n",
    "a. The ERD tool from the PGAdmin software was used to create the tables and their relationships\n",
    "b. The SQL commands was generated after creating the ERD\n",
    "c. Connect to your psql instance and run the SQL commands to create the tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the original dataset, the following tables will be constructed:\n",
    "\n",
    "1. dateDimTable - information on the date of the sale such as year, month, day, quarter, etc.\n",
    "2. sellerDimtable - seller information such as seller name and seller id\n",
    "3. stateDimTable - state information such as state abbreviation and state id\n",
    "4. vehicleDimTable - information on vehicle's year, make, body, transmission, etc.\n",
    "5. salesFactTable - vehicle id, date id, seller id, state id, selling price, mmr, odometer, condition, vin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the generated SQL command for creating the tables and their relationships. Run this code in the terminal while connected to your PSQL instance.\n",
    "\n",
    "-- This script was generated by the ERD tool in pgAdmin 4.\n",
    "-- Please log an issue at https://github.com/pgadmin-org/pgadmin4/issues/new/choose if you find any bugs, including reproduction steps.\n",
    "BEGIN;\n",
    "\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS public.\"dateDimTable\"\n",
    "(\n",
    "    date_id integer NOT NULL,\n",
    "    saledate date NOT NULL,\n",
    "    saledate_year integer NOT NULL,\n",
    "    saledate_month integer NOT NULL,\n",
    "    saledate_monthname character varying(20) NOT NULL,\n",
    "    saledate_day integer NOT NULL,\n",
    "    saledate_weekday integer NOT NULL,\n",
    "    saledate_weekdayname character varying(20) NOT NULL,\n",
    "    quarter integer NOT NULL,\n",
    "    quartername character varying(2) NOT NULL,\n",
    "    PRIMARY KEY (date_id)\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS public.\"vehicleDimTable\"\n",
    "(\n",
    "    vehicle_id integer NOT NULL,\n",
    "    year integer NOT NULL,\n",
    "    make character varying(30) NOT NULL,\n",
    "    model character varying(30) NOT NULL,\n",
    "    \"trim\" character varying(100) NOT NULL,\n",
    "    body character varying(30) NOT NULL,\n",
    "    transmission character varying(30) NOT NULL,\n",
    "    color character varying(30) NOT NULL,\n",
    "    interior character varying(30) NOT NULL,\n",
    "    PRIMARY KEY (vehicle_id)\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS public.\"sellerDimTable\"\n",
    "(\n",
    "    seller_id integer NOT NULL,\n",
    "    seller character varying(50) NOT NULL,\n",
    "    PRIMARY KEY (seller_id)\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS public.\"stateDimTable\"\n",
    "(\n",
    "    state_id integer NOT NULL,\n",
    "    state character varying(5) NOT NULL,\n",
    "    PRIMARY KEY (state_id)\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS public.\"salesFactTable\"\n",
    "(\n",
    "    sale_id integer NOT NULL,\n",
    "    vin character varying(50) NOT NULL,\n",
    "    vehicle_id integer NOT NULL,\n",
    "    state_id integer NOT NULL,\n",
    "    seller_id integer NOT NULL,\n",
    "    mmr numeric(9, 2) NOT NULL,\n",
    "    sellingprice numeric(9, 2) NOT NULL,\n",
    "    odometer numeric(11, 2) NOT NULL,\n",
    "    condition integer NOT NULL,\n",
    "    date_id integer NOT NULL,\n",
    "    PRIMARY KEY (sale_id)\n",
    ");\n",
    "\n",
    "ALTER TABLE IF EXISTS public.\"salesFactTable\"\n",
    "    ADD FOREIGN KEY (vehicle_id)\n",
    "    REFERENCES public.\"vehicleDimTable\" (vehicle_id) MATCH SIMPLE\n",
    "    ON UPDATE NO ACTION\n",
    "    ON DELETE NO ACTION\n",
    "    NOT VALID;\n",
    "\n",
    "\n",
    "ALTER TABLE IF EXISTS public.\"salesFactTable\"\n",
    "    ADD FOREIGN KEY (state_id)\n",
    "    REFERENCES public.\"stateDimTable\" (state_id) MATCH SIMPLE\n",
    "    ON UPDATE NO ACTION\n",
    "    ON DELETE NO ACTION\n",
    "    NOT VALID;\n",
    "\n",
    "\n",
    "ALTER TABLE IF EXISTS public.\"salesFactTable\"\n",
    "    ADD FOREIGN KEY (seller_id)\n",
    "    REFERENCES public.\"sellerDimTable\" (seller_id) MATCH SIMPLE\n",
    "    ON UPDATE NO ACTION\n",
    "    ON DELETE NO ACTION\n",
    "    NOT VALID;\n",
    "\n",
    "\n",
    "ALTER TABLE IF EXISTS public.\"salesFactTable\"\n",
    "    ADD FOREIGN KEY (date_id)\n",
    "    REFERENCES public.\"dateDimTable\" (date_id) MATCH SIMPLE\n",
    "    ON UPDATE NO ACTION\n",
    "    ON DELETE NO ACTION\n",
    "    NOT VALID;\n",
    "\n",
    "END;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the command '\\dt' to check the tables. There should be 5 tables:\n",
    "\n",
    "1. dateDimTable\n",
    "2. sellerDimtable \n",
    "3. stateDimTable \n",
    "4. vehicleDimTable \n",
    "5. salesFactTable "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4.Create an ETL/ELT pipeline\n",
    "\n",
    "a. Install pandas\n",
    "b. Read the dataset using pandas and make a dataframe\n",
    "c. Handle duplicate values\n",
    "d. Impute missing values\n",
    "e. Create additional columns in the dataframe suitable for the starr schema in PSQL\n",
    "f. Create multiple dataframes from the original dataframe mirroring the schema of the tables in PSQL and load them to csv files\n",
    "g. Add unique IDs for each dataframes\n",
    "h. Load the dataframes into CSV files\n",
    "i. Import the CSV files into PSQL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install pandas\n",
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the dataset into a pandas dataframe\n",
    "df = pd.read_csv(r'~/bluedrive_project/Vehicle_sales_data.csv')\n",
    "#get the initial length of the df\n",
    "start_length = len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#handle duplicates\n",
    "#the vin column is a good way to check for duplicates since this should be unique per vehicle\n",
    "df.drop_duplicates(subset='vin', inplace=True)\n",
    "#get the length of the df after dropping duplicates\n",
    "length_after_duplicates = len(df)\n",
    "print(f'Dropped Rows:{start_length-length_after_duplicates} ({(start_length-length_after_duplicates)*100/start_length}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's try to see what will happen if we drop all rows with missing values\n",
    "#make a copy of df\n",
    "df_test = df.copy()\n",
    "df_test.dropna(inplace=True)\n",
    "length_after_dropping_all_na = len(df_test)\n",
    "print(f'Dropped Rows:{start_length-length_after_dropping_all_na} ({(start_length-length_after_dropping_all_na)*100/start_length}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen, almost 16.8 % of the data was dropped because of dropping all missing values. For this reason, a strategy for imputation of missing values will be implemented.\n",
    "\n",
    "1. Impute missing values with the mode value of the target column referenced to a relevant column\n",
    "2. Impute missing values with the mean value of the target column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define two functions, one for imputing missing values with the mode value, and another one for imputing missing values with the mean\n",
    "\n",
    "#function to return a df where missing values are imputed using the mode based on a relevant column\n",
    "def fillna_with_mode(target_col, reference_col, dataframe):\n",
    "    \n",
    "    most_freq = dataframe.groupby(reference_col)[target_col] \\\n",
    "        .agg(lambda x: x.mode()[0] if not x.mode().empty else 'Unknown')\n",
    "    most_freq = most_freq.reset_index(name = 'most_freq')\n",
    "    \n",
    "    merged = pd.merge(dataframe, most_freq, on=reference_col, how='left')\n",
    "    dataframe[target_col] = dataframe[target_col].fillna(merged['most_freq'])\n",
    "    return dataframe\n",
    "\n",
    "#function to impute missing values in the target column with the mean value of the target column\n",
    "def fillna_with_mean(target_col, dataframe):\n",
    "    \n",
    "    mean_value = int(dataframe[target_col].mean())\n",
    "    dataframe[target_col] = dataframe[target_col].fillna(mean_value)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#handle missing values for the make column\n",
    "#fill missing make values using mode of make grouped by seller\n",
    "df = fillna_with_mode('make', 'seller', df)\n",
    "df.dropna(subset='make', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill missing model values using mode of model grouped by make\n",
    "df = fillna_with_mode('model', 'make', df)\n",
    "df.dropna(subset='model', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill missing trim values using mode of trim grouped by model\n",
    "df = fillna_with_mode('trim', 'model', df)\n",
    "df.dropna(subset='trim', inplace=True)\n",
    "\n",
    "#fill missing body values using mode of body grouped by model\n",
    "df = fillna_with_mode('body', 'model', df)\n",
    "df.dropna(subset='body', inplace=True)\n",
    "\n",
    "#fill missing transmission values using mode of transmission grouped by model\n",
    "df = fillna_with_mode('transmission', 'model', df)\n",
    "df.dropna(subset='transmission', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill missing values in the condition column with the mean value of the condition column\n",
    "df = fillna_with_mean('condition', df)\n",
    "df.dropna(subset='condition', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop rows for other missing values and incorrect input\n",
    "\n",
    "#drop state values where length of input is more than 2\n",
    "df = df[df['state'].str.len() == 2]\n",
    "\n",
    "#drop rows with missing odometer values\n",
    "df.dropna(subset='odometer', inplace=True)\n",
    "\n",
    "#drop rows with missing color values and — values\n",
    "df.dropna(subset='color', inplace=True)\n",
    "df = df[df['color'] != '—']\n",
    "\n",
    "#drop rows with missing mmr values\n",
    "df.dropna(subset='mmr', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the length of the df after the imputations and drops\n",
    "end_length = len(df)\n",
    "print(f'Dropped Rows:{start_length-end_length} ({(start_length-end_length)*100/start_length}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen, the dropped rows are now just 6.3%. We managed to bring it down from 16.8%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the code below will expand the saledate column in preparation for the star schema tables\n",
    "\n",
    "def generate_weekday(dataframe):\n",
    "    weekday_map = {'Mon':1, 'Tue':2, 'Wed':3, 'Thu':4, 'Fri':5, 'Sat':6, 'Sun':7}\n",
    "    dataframe['saledate_weekday'] = dataframe['saledate_weekdayname'].map(weekday_map)\n",
    "    return dataframe\n",
    "\n",
    "#parse the saledate column to get just the date part and store in a new column\n",
    "df['saledate_new'] = df['saledate'].str[4:15]\n",
    "#parse the saledate column to get just the weekdayname and store in a new column\n",
    "df['saledate_weekdayname'] = df['saledate'].str[0:3]\n",
    "#add anew column saledate_weekday corresponding to weekdayname, Mon = 1, Tue = 2, etc.\n",
    "df = generate_weekday(df)\n",
    "#format saledate_new to datetime\n",
    "df['saledate_new'] = pd.to_datetime(df['saledate_new'])\n",
    "#drop the saledate column then rename saledate_new to saledate\n",
    "df.drop('saledate', axis=1, inplace=True)\n",
    "df.rename(columns={'saledate_new': 'saledate'}, inplace=True)\n",
    "#add a column for year, month and day\n",
    "df['saledate_year'] = df['saledate'].dt.year\n",
    "df['saledate_month'] = df['saledate'].dt.month\n",
    "df['saledate_monthname'] = df['saledate'].dt.month_name()\n",
    "df['saledate_day'] = df['saledate'].dt.day\n",
    "#add a quarter and a quartername column\n",
    "df['quarter'] = df['saledate'].dt.quarter\n",
    "df['quartername'] = 'Q' + df['quarter'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the code below will cast the correct data types for ecah columns\n",
    "\n",
    "#cast correct data types for each columns\n",
    "df['year'] = df['year'].astype(int)\n",
    "df['make'] = df['make'].astype('string')\n",
    "df['model'] = df['model'].astype('string')\n",
    "df['trim'] = df['trim'].astype('string')\n",
    "df['body'] = df['body'].astype('string')\n",
    "df['transmission'] = df['transmission'].astype('string')\n",
    "df['vin'] = df['vin'].astype('string')\n",
    "df['state'] = df['state'].astype('string')\n",
    "df['condition'] = df['condition'].astype(int)\n",
    "df['odometer'] = df['odometer'].astype(float)\n",
    "df['color'] = df['color'].astype('string')\n",
    "df['interior'] = df['interior'].astype('string')\n",
    "df['seller'] = df['seller'].astype('string')\n",
    "df['mmr'] = df['mmr'].astype(float)\n",
    "df['sellingprice'] = df['sellingprice'].astype(float)\n",
    "df['saledate'] = pd.to_datetime(df['saledate'])\n",
    "df['saledate_year'] = df['saledate_year'].astype(int)\n",
    "df['saledate_month'] = df['saledate_month'].astype(int)\n",
    "df['saledate_monthname'] = df['saledate_monthname'].astype('string')\n",
    "df['saledate_day'] = df['saledate_day'].astype(int)\n",
    "df['saledate_weekdayname'] = df['saledate_weekdayname'].astype('string')\n",
    "df['saledate_weekday'] = df['saledate_weekday'].astype(int)\n",
    "df['quarter'] = df['quarter'].astype(int)\n",
    "df['quartername'] = df['quartername'].astype('string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final check for missing values before preparing the dataframes for the star schema\n",
    "#columns with missing values will be True, otherwise, False\n",
    "print(f'Number of Rows: {len(df)}')\n",
    "print(df.isnull().any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next steps in the ETL process will be the creation of multiple dataframes that will mirror the schema of the tables prepared earlier in the PSQL instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code for generating unique IDs for the multiple dataframes\n",
    "\n",
    "def generate_id(dataframe, target_col):\n",
    "    unique_values = dataframe[target_col].unique()\n",
    "    ctr = 1\n",
    "    my_dict = {}\n",
    "    for x in unique_values:\n",
    "        my_dict[x] = ctr\n",
    "        ctr = ctr+1\n",
    "    return my_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dateDimTable\n",
    "\n",
    "dateDimTable = df[['saledate', 'saledate_year', 'saledate_month', 'saledate_monthname','saledate_day',\n",
    "                   'saledate_weekday', 'saledate_weekdayname', 'quarter', 'quartername', \n",
    "                   ]]\n",
    "dateDimTable = dateDimTable.copy()\n",
    "dateDimTable['date_id'] = dateDimTable['saledate'].map(generate_id(dateDimTable, 'saledate'))\n",
    "dateDimTable.drop_duplicates(inplace=True)\n",
    "dateDimTable.to_csv('~/bluedrive_project/dateDimTable.csv', index=False)\n",
    "\n",
    "#sellerDimTable\n",
    "sellerDimTable = df[['seller']]\n",
    "sellerDimTable = sellerDimTable.copy()\n",
    "sellerDimTable['seller_id'] = sellerDimTable['seller'].map(generate_id(sellerDimTable, 'seller'))\n",
    "sellerDimTable.drop_duplicates(inplace=True)\n",
    "sellerDimTable.to_csv('~/bluedrive_project/sellerDimTable.csv', index=False)\n",
    "\n",
    "#stateDimTable\n",
    "stateDimTable = df[['state']]\n",
    "stateDimTable = stateDimTable.copy()\n",
    "stateDimTable['state_id'] = stateDimTable['state'].map(generate_id(stateDimTable, 'state'))\n",
    "stateDimTable.drop_duplicates(inplace=True)\n",
    "stateDimTable.to_csv('~/bluedrive_project/stateDimTable.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vehicleDimTable\n",
    "#slightly different than the first 3 above, the uniuqe id of vehicleDimTable is referenced to the uniuqe combination \n",
    "#of its columns and not just one column\n",
    "vehicleDimTable = df[['year', 'make', 'model', 'trim', 'body', 'transmission', 'color', 'interior']].drop_duplicates()\n",
    "vehicleDimTable = vehicleDimTable.copy()\n",
    "vehicleDimTable['vehicle_id'] = vehicleDimTable.reset_index().index+1\n",
    "vehicleDimTable.to_csv('~/bluedrive_project/vehicleDimTable.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#salesFactTable\n",
    "#merge the salesFactTable with the DimTables to get the incorporate the Dimtables' ID in the salesFactTable\n",
    "salesFactTable = df.copy()\n",
    "salesFactTable = salesFactTable.merge(dateDimTable[['date_id', 'saledate']], on='saledate', how='left')\n",
    "salesFactTable = salesFactTable.merge(sellerDimTable[['seller_id', 'seller']], on='seller', how='left')\n",
    "salesFactTable = salesFactTable.merge(stateDimTable[['state_id', 'state']], on='state', how='left')\n",
    "salesFactTable = salesFactTable.merge(vehicleDimTable[['vehicle_id', 'year', 'make', 'model', 'trim', 'body', \n",
    "                                                       'transmission', 'color', 'interior']], \n",
    "                                      on=['year', 'make', 'model', 'trim', 'body', 'transmission', 'color', 'interior'], \n",
    "                                      how='left')\n",
    "\n",
    "#Drop Redundant Columns (i.e., columns that are now part of the dimension tables) leaving the Dimtables IDs only and some other fact columns\n",
    "salesFactTable = salesFactTable.drop(columns=['year', 'make', 'model', 'trim', 'body', 'transmission',\n",
    "                                              'color', 'interior', 'saledate', 'seller', 'state',\n",
    "                                              'saledate_weekdayname','saledate_weekday', 'saledate_year',\n",
    "                                              'saledate_month', 'saledate_monthname', 'saledate_day',\n",
    "                                              'quarter', 'quartername'])\n",
    "salesFactTable['sale_id'] = salesFactTable.reset_index().index+1\n",
    "\n",
    "\n",
    "salesFactTable.to_csv('~/bluedrive_project/salesFactTable.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, there should be 5 CSV files in your project directory:\n",
    "\n",
    "1. dateDimTable.csv\n",
    "2. sellerDimtable.csv\n",
    "3. stateDimTable.csv\n",
    "4. vehicleDimTable.csv\n",
    "5. salesFactTable.csv\n",
    "\n",
    "The final step is to load these CSV files into their respective tables in the PSQL instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the following:\n",
    "\n",
    "1. Start the PSQL instnace in docker\n",
    "2. Connect to your PSQL instance\n",
    "3. Import the CSV files into the tables using the \\COPY method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the CSV files to the PSQL instance:\n",
    "\n",
    "\n",
    "\\copy \"dateDimTable\"(saledate,saledate_year,saledate_month,saledate_monthname,saledate_day,saledate_weekday,saledate_weekdayname,quarter,quartername,date_id) FROM '~/bluedrive_project/dateDimTable.csv' DELIMITER ',' CSV HEADER;\n",
    "\n",
    "\\copy \"sellerDimTable\"(seller,seller_id) FROM '~/bluedrive_project/sellerDimTable.csv' DELIMITER ',' CSV HEADER;\n",
    "\n",
    "\\copy \"stateDimTable\"(state,state_id) FROM '~/bluedrive_project/stateDimTable.csv' DELIMITER ',' CSV HEADER;\n",
    "\n",
    "\\copy \"vehicleDimTable\"(year,make,model,trim,body,transmission,color,interior,vehicle_id) FROM '~/bluedrive_project/vehicleDimTable.csv' DELIMITER ',' CSV HEADER;\n",
    "\n",
    "\\copy \"salesFactTable\"(vin,condition,odometer,mmr,sellingprice,date_id,seller_id,state_id,vehicle_id,sale_id) FROM '~/bluedrive_project/salesFactTable.csv' DELIMITER ',' CSV HEADER;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the query below to verify your the integrity of your tables:\n",
    "\n",
    "SELECT v.year, v.make, v.model, v.trim, v.body, v.transmission, s.vin, st.state, s.condition, s.odometer, v.color, v.interior, sel.seller, s.sellingprice, s.mmr, d.saledate  FROM \"salesFactTable\" AS s LEFT JOIN \"dateDimTable\" AS d ON s.date_id = d.date_id LEFT JOIN \"sellerDimTable\" AS sel ON sel.seller_id = s.seller_id LEFT JOIN \"stateDimTable\" AS st ON st.state_id = s.state_id LEFT JOIN \"vehicleDimTable\" AS v ON v.vehicle_id = s.vehicle_id  LIMIT 1;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The query should output the first row in the original dataset csv file which is\n",
    "\n",
    "year | make |  model  | trim | body | transmission |        vin        | state | condition | odometer | color | interior |         seller          | sellingprice |   mmr    |  saledate  \n",
    "------+------+---------+------+------+--------------+-------------------+-------+-----------+----------+-------+----------+-------------------------+--------------+----------+------------\n",
    " 2015 | Kia  | Sorento | LX   | SUV  | automatic    | 5xyktca69fg566472 | ca    |         5 | 16639.00 | white | black    | kia motors america  inc |     21500.00 | 20500.00 | 2014-12-16"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
